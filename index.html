<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>About me - Suorong Yang</title>
  <style>
    .content ul li {
      margin-bottom: 18px;     /* ÊØèÊù°‰πãÈó¥ÁöÑÈó¥Ë∑ù */
      line-height: 1.6;        /* Ë°åË∑ù */
    }

    .content ul {
      margin-left: 0;
      padding-left: 20px;      /* Áº©Ëøõ‰øùÊåÅ */
      list-style-type: disc;   /* È°πÁõÆÂâçÁöÑÂúÜÁÇπÔºàÂèØÊîπ‰∏∫ noneÔºâ */
    }
    body {
      margin: 0;
      font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
      color: #333;
      line-height: 1.6;
      display: flex;
      min-height: 100vh;
    }

    /* Sidebar */
    .sidebar {
      background-color: #f5f5f5;
      width: 280px;
      padding: 40px 25px;
      box-sizing: border-box;
      flex-shrink: 0;
      border-right: 1px solid #ddd;
    }

    .sidebar img {
      width: 120px;
      height: 120px;
      border-radius: 10%;
      display: block;
      margin: 0 auto 20px;
    }

    .sidebar h1 {
      text-align: center;
      font-size: 20px;
      margin-bottom: 5px;
    }

    .sidebar p {
      text-align: center;
      font-size: 14px;
      color: #555;
      margin: 4px 0;
    }

    .sidebar a {
      color: #0066cc;
      text-decoration: none;
    }

    .sidebar a:hover {
      text-decoration: underline;
    }

    .sidebar .links {
      text-align: center;
      margin-top: 20px;
    }

    .sidebar .links a {
      display: block;
      margin: 5px 0;
      font-size: 14px;
    }

    /* Main Content */
    .content {
      flex-grow: 1;
      padding: 40px 60px;
      max-width: 850px;
      margin: auto;
    }

    h2 {
      border-bottom: 2px solid #eee;
      padding-bottom: 5px;
      margin-top: 40px;
    }

    ul {
      padding-left: 20px;
    }

    a {
      color: #0066cc;
      text-decoration: none;
    }

    a:hover {
      text-decoration: underline;
    }

    /* Responsive */
    @media (max-width: 800px) {
      body {
        flex-direction: column;
      }
      .sidebar {
        width: 100%;
        border-right: none;
        border-bottom: 1px solid #ddd;s
      }
      .content {
        padding: 20px;
      }
    }
  </style>
</head>
<body>

  <!-- Sidebar -->
  <div class="sidebar">
    <img src="https://raw.githubusercontent.com/suorongyang/suorongyang.github.io/main/fig.png"
    alt="Profile Photo"
    width="300"
    style="height:auto; object-fit:cover;">
    <h1>Suorong Yang</h1>
    <p>CS Ph.D. Candidate</p>
    <p>Nanjing University</p>
    <div class="links">
      <a href="mailto:sryang@smail.nju.edu.cn">üìß Email</a>
      <a href="https://scholar.google.com/citations?user=baP7X_YAAAAJ&hl=en&authuser=1">üéì Google Scholar</a>
      <a href="https://github.com/Jackbrocp">üíª GitHub</a>
      <a href="https://x.com/Suorong_Yang">üîó Twitter</a>
    </div>
  </div>

  <!-- Main Content -->
  <div class="content">
    <h2>About Me</h2>
    <p>
      I am a final-year PhD candidate at <a href="https://www.nju.edu.cn/en/">Nanjing University</a>, advised by <b>Prof. Furao Shen</b>. 
      Currently, I am a research intern at the Shanghai AI Lab, where I collaborated with Prof. <a href="https://wlouyang.github.io/">Wanli Ouyang</a> and Dr. Dongzhan Zhou.
      Previously, I was a visiting scholar at the National University of Singapore, working with Prof. <a href="https://www.comp.nus.edu.sg/~youy/">Yang You</a>.
      I also work closely with Prof. <a href="https://tongliang-liu.github.io/">Tongliang Liu</a> at The University of Sydney, Prof. <a href="https://faculty.ustc.edu.cn/dongeliu/">Dong Liu</a> at the University of Science and Technology of China, 
      and Prof. <a href="https://ese.nju.edu.cn/zj_en/list.htm">Jian Zhao</a> at Nanjing University.
    </p>
    <p>
      My primary research focuses on data-centric AI, multimodal learning, and the interpretability of LLMs. At present, most of my work are dedicated to improving the efficiency, scalability,
      and robustness of large deep learning models by fully unleashing the full potential of data.
    </p>

    <h2>üì∞ News</h2>
<ul>
  <li><b>Oct. 2025:</b> Give a talk at <b>ICCV 2025 CDEL Workshop</b>. Great discussion on data-centric learning!</li>
  <li><b>Aug. 2025:</b> <a href="https://ieeexplore.ieee.org/document/11175550" target="_blank">IPF-RDA</a> has been accepted by <b>IEEE TPAMI</b>! Congrats to all co-authors!</li>
  <li><b>Jul. 2025:</b> <a href="https://arxiv.org/abs/2507.05397" target="_blank">Neural-Driven Image Editing</a> has been accepted by <b>NeurIPS 2025</b>! Congrats to Pengfei!</li>
  <li><b>Jul. 2025:</b> Give a talk at A*STAR CFAR.</li>
  <li><b>May. 2025:</b> <a href="https://arxiv.org/pdf/2506.21037" target="_blank">RL-Selector</a> has been accepted by <b>ICCV 2025</b>. Congrats to all collaborators!</li>
  <li><b>May. 2025:</b> Got one paper accepted by <b>ICML 2025</b>. Congrats to all collaborators!</li>
  <li><b>Feb. 2025:</b> Got one paper accepted by <b>CVPR 2025</b>. Congratulations to Di, Junxian, Jingdi, and Xunzhi!</li>
  <li><b>Jan. 2025:</b> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608024007287" target="_blank">GMNI</a> accepted by <b>Neural Networks</b>. Congratulations to Xin and Xiangyu!</li>
  <li><b>Jan. 2025:</b> <em>A CLIP-Powered Framework for Robust and Generalizable Data Selection</em> has been accepted by <b>ICLR 2025 (Spotlight)</b>! üéâ</li>
  <li><b>Dec. 2024:</b> Got two papers accepted by <b>Neural Network</b>. Congrats to Hongchao!</li>
  <li><b>Aug. 2024:</b> <a href="https://link.springer.com/chapter/10.1007/978-3-031-72848-8_12" target="_blank">EntAugment</a> has been accepted by <b>ECCV 2024</b>. Congrats to collaborators!</li>
  <li><b>Aug. 2024:</b> Got one paper accepted by Pattern Recognition. Congrats to collaborators!</li>
  <li><b>Dec. 2023:</b> Got one paper accepted by Pattern Recognition. Congrats to collaborators!</li>
  
  
</ul>


    <h2>Selected Publications</h2>
<ul>

  <li>
    <b>IPF-RDA: An Information-Preserving Framework for Robust Data Augmentation.</b><br>
    <b>Suorong Yang</b>, Hongchao Yang, Suhan Guo, Furao Shen, and Jian Zhao.<br>
    <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34" target="_blank">TPAMI</a>)</i>. 
    [<a href="https://ieeexplore.ieee.org/document/11175550" target="_blank">PDF</a>] 
    [<a href="https://github.com/Jackbrocp/IPF-RDA" target="_blank">CODE</a>]
  </li>

  <li>
    <b>A CLIP-Powered Framework for Robust and Generalizable Data Selection.</b><br>
    <b>Suorong Yang</b>, Peng Ye, Wanli Ouyang, Dongzhan Zhou, and Furao Shen.<br>
    <i>International Conference on Learning Representations (<a href="https://iclr.cc/" target="_blank">ICLR</a>)</i> 2025. 
    [<a href="https://openreview.net/forum?id=9bMZ29SPVx" target="_blank">PDF</a>] 
    [<a href="https://github.com/Jackbrocp/clip-powered-data-selection" target="_blank">CODE</a>]
    <br>
  <span style="color:#555; font-size:14px;">
    (This paper was selected for <b>Spotlight Presentation</b>, acceptance rate: <b>5%</b>.)
  </span>
  </li>

  <li>
    <b>When Dynamic Data Selection Meets Data Augmentation: Achieving Enhanced Training Acceleration.</b><br>
    <b>Suorong Yang</b>, Peng Ye, Furao Shen, and Dongzhan Zhou.<br>
    <i>International Conference on Machine Learning (<a href="https://icml.cc/" target="_blank">ICML</a>)</i> 2025. 
    [<a href="https://arxiv.org/pdf/2505.03809" target="_blank">PDF</a>]
  </li>

  <li>
    <b>AdaAugment: A Tuning-Free and Adaptive Approach to Enhance Data Augmentation.</b><br>
    <b>Suorong Yang</b>, Peijia Li, Furao Shen, and Jian Zhao.<br>
    <i>IEEE Transactions on Image Processing (<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?reload=true&punumber=83" target="_blank">TIP</a>)</i>. 
    [<a href="https://ieeexplore.ieee.org/document/11104992" target="_blank">PDF</a>] 
    [<a href="https://github.com/Jackbrocp/AdaAugment" target="_blank">CODE</a>]
  </li>

  <li>
    <b>RL-Selector: Reinforcement Learning-Guided Data Selection via Redundancy Assessment.</b><br>
    <b>Suorong Yang</b>, Peijia Li, Furao Shen, and Jian Zhao.<br>
    <i>IEEE/CVF International Conference on Computer Vision (<a href="https://iccv.thecvf.com/" target="_blank">ICCV</a>)</i> 2025. 
    [<a href="https://arxiv.org/pdf/2506.21037" target="_blank">PDF</a>]
  </li>

  <li>
    <b>EntAugment: Entropy-Driven Adaptive Data Augmentation Framework for Enhancing Generalization in Image Classification.</b><br>
    <b>Suorong Yang</b>, Furao Shen, and Jian Zhao.<br>
    <i>European Conference on Computer Vision (<a href="https://eccv.ecva.net/" target="_blank">ECCV</a>)</i> 2024. 
    [<a href="https://eccv.ecva.net/virtual/2024/poster/819" target="_blank">PDF</a>] 
    [<a href="https://github.com/Jackbrocp/EntAugment" target="_blank">CODE</a>]
  </li>

  <!-- <li>
    <b>Multimodal-Guided Dynamic Dataset Pruning for Robust and Generalizable Data-Centric Learning.</b><br>
    <b>Suorong Yang</b>, Peijia Li, Yujie Liu, Xu Zhiming, Peng Ye, Wanli Ouyang, Furao Shen, and Dongzhan Zhou.<br>
    <i>International Conference on Machine Learning (ICML)</i> 2025, <b>DataWorld Workshop</b>.
  </li> -->

  <li>
    <b>AdvMask: A Sparse Adversarial Attack-Based Data Augmentation Method for Image Classification.</b><br>
    <b>Suorong Yang</b>, Jinqiao Li, Tianyue Zhang, Jian Zhao, and Furao Shen (2023).<br>
    <i>Pattern Recognition (<a href="https://www.sciencedirect.com/journal/pattern-recognition" target="_blank">PR</a>)</i>, 144, 109847. 
    [<a href="https://www.sciencedirect.com/science/article/pii/S0031320323005459/pdfft?md5=17829582bcd775213d24ea26ba49d773&pid=1-s2.0-S0031320323005459-main.pdf" target="_blank">PDF</a>] 
    [<a href="https://github.com/Jackbrocp/AdvMask" target="_blank">CODE</a>]
  </li>

  <li>
    <b>Investigating the Effectiveness of Data Augmentation from Similarity and Diversity: An Empirical Study.</b><br>
    <b>Suorong Yang</b>, Hongchao Yang, Suhan Guo, Furao Shen, and Jian Zhao.<br>
    <i>Pattern Recognition (<a href="https://www.sciencedirect.com/journal/pattern-recognition" target="_blank">PR</a>)</i>. 
    [<a href="https://www.sciencedirect.com/science/article/pii/S0031320323009019" target="_blank">PDF</a>] 
    [<a href="https://github.com/Jackbrocp/Investigating_the_Effectiveness_of_Data_Augmentation" target="_blank">CODE</a>]
  </li>

  <li>
    <b>Supervised Contrastive Learning with Prototype Distillation for Data Incremental Learning.</b><br>
    <b>Suorong Yang</b>, Tianyue Zhang, Zhiming Xu, Leijia Li, Baile Xu, Furao Shen, and Jian Zhao.<br>
    <i>Neural Networks (<a href="https://www.sciencedirect.com/journal/neural-networks" target="_blank">NN</a>)</i>. 
    [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608025005313" target="_blank">PDF</a>]
  </li>

  <li>
    <b>Critic-V: VLM Critics Help Catch VLM Errors in Multimodal Reasoning.</b><br>
    Di Zhang, Junxian Li, Jingdi Lei, Xunzhi Wang, Yujie Liu, Zonglin Yang, Jiatong Li, Weida Wang, 
    <b>Suorong Yang</b>, Jianbo Wu, Peng Ye, Wanli Ouyang, and Dongzhan Zhou.<br>
    <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a href="https://www.sciencedirect.com/journal/pattern-recognition" target="_blank">CVPR</a>)</i> 2025. 
    [<a href="https://arxiv.org/abs/2411.18203" target="_blank">PDF</a>]
  </li>
  
  <li>
    <b>CS-QCFS: Bridging the Performance Gap in Ultra-Low Latency Spiking Neural Networks.</b><br>
    Hongchao Yang, <b>Suorong Yang</b>, Jian Zhao, and Furao Shen.<br>
    <i>Neural Networks (<a href="https://www.sciencedirect.com/journal/neural-networks" target="_blank">NN</a>)</i>, Vol. 184, 107076 (2025). 
    [<a href="https://www.sciencedirect.com/science/article/abs/pii/S0893608024010050" target="_blank">PDF</a>]
  </li>
  
  <li>
    <b>Sensitivity Pruner: Filter-Level Compression Algorithm for Deep Neural Networks.</b><br>
    Suhan Guo, Bilan Lai, <b>Suorong Yang</b>, Jian Zhao, and Furao Shen (2023).<br>
    <i>Pattern Recognition (<a href="https://www.sciencedirect.com/journal/pattern-recognition" target="_blank">PR</a>)</i>, Vol. 140, 109508. 
    [<a href="https://www.sciencedirect.com/science/article/pii/S003132032300208X/pdfft?md5=ac1bd1175750c03a3826a7e8ddd2d8d9&pid=1-s2.0-S003132032300208X-main.pdf" target="_blank">PDF</a>] 
    [<a href="https://github.com/SuhanG17/Sensitivity_Pruner" target="_blank">CODE</a>]
  </li>
  
  <li>
    <b>GMNI: Achieve Good Data Augmentation in Unsupervised Graph Contrastive Learning.</b><br>
    Xiong X., Wang X., <b>Suorong Yang</b>, Furao Shen, and Jian Zhao.<br>
    <i>Neural Networks (<a href="https://www.sciencedirect.com/journal/neural-networks" target="_blank">NN</a>)</i>, Vol. 181 (2025): 106804. 
    [<a href="https://www.sciencedirect.com/science/article/pii/S0893608024007287" target="_blank">PDF</a>]
  </li>
  

</ul>



<h2>üìö Academic Service</h2>

<ul>
  <li>
    <strong>Reviewer</strong>
    <ul>
      <li>
        <b>Journals:</b> 
        <em>IEEE Transactions on Image Processing (TIP)</em>, 
        <em>International Journal of Computer Vision (IJCV)</em>, 
        <em>Neural Networks</em>, 
        <em>Pattern Recognition</em>, 
        <em>Information Fusion</em>, etc.
      </li>
      <li>
        <b>Conferences:</b> 
        <em>CVPR</em>, <em>ECCV</em>, <em>ICCV</em>, 
        <em>ICLR</em>, <em>ICML</em>, and <em>NeurIPS</em>, etc.
      </li>
    </ul>
  </li>
</ul>

  </div>

</body>
</html>
